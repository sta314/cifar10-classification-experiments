{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows the last state of our code after cumulative additions. More detailed code for each experiment can be reached from https://wandb.ai/takim/CIFAR-10_Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Data Loading Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_cifar10_batch(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        batch = pickle.load(file, encoding='bytes')\n",
    "    return batch\n",
    "\n",
    "def load_cifar10_data(folder_path):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        batch_file = f\"{folder_path}/data_batch_{i}\"\n",
    "        batch = load_cifar10_batch(batch_file)\n",
    "        train_data.append(batch[b'data'])\n",
    "        train_labels.extend(batch[b'labels'])\n",
    "\n",
    "    test_batch_file = f\"{folder_path}/test_batch\"\n",
    "    test_batch = load_cifar10_batch(test_batch_file)\n",
    "    test_data = test_batch[b'data']\n",
    "    test_labels = test_batch[b'labels']\n",
    "\n",
    "    train_data = np.vstack(train_data)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "def preprocess_data(train_data, train_labels, test_data, test_labels):\n",
    "    train_data = train_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    train_labels_onehot = to_categorical(train_labels)\n",
    "    test_labels_onehot = to_categorical(test_labels)\n",
    "\n",
    "    return train_data, train_labels_onehot, test_data, test_labels_onehot\n",
    "\n",
    "cifar10_folder = 'cifar-10-batches-py'\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar10_data(cifar10_folder)\n",
    "\n",
    "x_train, y_train, x_test, y_test = preprocess_data(\n",
    "    train_data, train_labels, test_data, test_labels\n",
    ")\n",
    "\n",
    "print(\"Train Data Shape:\", x_train.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Test Data Shape:\", x_test.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the class labels\n",
    "class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Function to plot sample images\n",
    "def plot_sample_images(images, labels):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(10):\n",
    "        axes[i].imshow(images[i])\n",
    "        axes[i].set_title(class_labels[labels[i]])\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot sample images\n",
    "plot_sample_images(x_train, np.argmax(y_train, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot class distribution of training data\n",
    "sns.countplot(x=np.argmax(y_train, axis=1))\n",
    "plt.title('Class Distribution - Training Data')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Plot class distribution of test data\n",
    "sns.countplot(x=np.argmax(y_test, axis=1))\n",
    "plt.title('Class Distribution - Test Data')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Flatten the image data\n",
    "flattened_data = x_train.flatten()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(flattened_data, bins=256, color='gray')\n",
    "plt.title('Pixel Value Distribution')\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights & Biases Related Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Resolution Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the usage of super resolution images is not given in this notebook, since they were not tested on some architectures. However, it is just a matter of replacing the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tqdm import tqdm\n",
    "\n",
    "supres_model = hub.load('https://tfhub.dev/captain-pool/esrgan-tf2/1')\n",
    "x_train_superres = supres_model(tf.cast(x_train[0:100], tf.float32), training=False).numpy().astype('uint8')\n",
    "for i in tqdm(range(100, x_train.shape[0], 100)):\n",
    "    result = supres_model(tf.cast(x_train[i:i+100], tf.float32), training=False).numpy().astype('uint8')\n",
    "    x_train_superres = np.concatenate((x_train_superres, result), axis=0)\n",
    "\n",
    "x_test_superres = supres_model(tf.cast(x_test[0:100], tf.float32), training=False).numpy().astype('uint8')\n",
    "for i in tqdm(range(100, x_test.shape[0], 100)):\n",
    "    result = supres_model(tf.cast(x_test[i:i+100], tf.float32), training=False).numpy().astype('uint8')\n",
    "    x_test_superres = np.concatenate((x_test_superres, result), axis=0)\n",
    "\n",
    "np.save('x_train_superres.npy', x_train_superres)\n",
    "np.save('x_test_superres.npy', x_test_superres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following blocks will have TWO merged blocks that contains the configuration and model definition for a specific architecture, run the correct configuration and model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'augmentation': {\n",
    "          'values': ['none', 'light', 'heavy']\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({\n",
    "    'earlystopping_patience': {\n",
    "        'value': 10},\n",
    "    'epochs': {\n",
    "        'value': 100},\n",
    "    'learning_rate': {\n",
    "        'value': 0.000063\n",
    "        },\n",
    "    'batch_size': {\n",
    "          'value': 64\n",
    "        },\n",
    "    'dropout': {\n",
    "          'value': True\n",
    "        },\n",
    "    'batchnorm': {\n",
    "          'value': True\n",
    "        },\n",
    "    'regularization': {\n",
    "          'value': False\n",
    "        },\n",
    "    'normalization': {\n",
    "        'value': True}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the AlexNet architecture\n",
    "def create_model(dropout, batchnorm, regularization):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    if regularization:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=96, kernel_size=(11, 11), strides=(2, 2), activation='relu', input_shape=(32, 32, 3), kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=96, kernel_size=(11, 11), strides=(2, 2), activation='relu', input_shape=(32, 32, 3)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "\n",
    "    if regularization:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding=\"same\"))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "\n",
    "    if regularization:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\"))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\"))\n",
    "        model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation='relu', padding=\"same\"))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu'))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu'))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'augmentation': {\n",
    "          'values': ['none', 'light', 'heavy']\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({\n",
    "    'earlystopping_patience': {\n",
    "        'value': 10},\n",
    "    'epochs': {\n",
    "        'value': 100},\n",
    "    'learning_rate': {\n",
    "        'value': 0.00025118864\n",
    "        },\n",
    "    'batch_size': {\n",
    "          'value': 64\n",
    "        },\n",
    "    'kernel_size': {\n",
    "        'value': (3, 3)\n",
    "        },\n",
    "    'dropout': {\n",
    "          'value': True\n",
    "        },\n",
    "    'pooling': {\n",
    "          'value': 'max'\n",
    "        },\n",
    "    'batchnorm': {\n",
    "          'value': True\n",
    "        },\n",
    "    'a_layers': {\n",
    "          'value': 16\n",
    "        },\n",
    "    'reg_alpha': {\n",
    "        'value': 0\n",
    "        },\n",
    "    'normalization': {\n",
    "        'value': False}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_model(kernel_size, dropout, pooling, batchnorm, n_layers, reg_alpha):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size, activation='relu', padding='same', input_shape=(32, 32, 3), kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if batchnorm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    if pooling == 'max':\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    if n_layers == 19:\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        model.add(tf.keras.layers.Conv2D(512, kernel_size, activation='relu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "        if batchnorm:\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=4096, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_alpha)))\n",
    "    if dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'augmentation': {\n",
    "          'values': ['none', 'light', 'heavy']\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({\n",
    "    'earlystopping_patience': {\n",
    "        'value': 10},\n",
    "    'epochs': {\n",
    "        'value': 1},\n",
    "    'learning_rate': {\n",
    "        'value': 0.000016},\n",
    "    'batch_size': {\n",
    "        'value': 128},\n",
    "    'reg_alpha': {\n",
    "          'value': 0.00001\n",
    "        },\n",
    "    'del_5x5': {\n",
    "          'value': False\n",
    "        },\n",
    "    'add_7x7': {\n",
    "          'value': False\n",
    "        },\n",
    "    'normalization': {\n",
    "          'value': True}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, Lambda, Add, Input, GlobalAveragePooling2D, Flatten, Dense, Softmax, MaxPooling2D, AveragePooling2D, Dropout\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def InceptionBlock(x, f1, f3_reduce, f3, f5_reduce, f5, pool_reduce, reg_alpha, del_5x5, add_7x7):\n",
    "\n",
    "    f7_reduce = f5_reduce\n",
    "    f7 = f5\n",
    "\n",
    "    p1_x = Conv2D(filters=f1, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(x)\n",
    "    p1_x = ReLU()(p1_x)\n",
    "\n",
    "    p3_x = Conv2D(filters=f3_reduce, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(x)\n",
    "    p3_x = ReLU()(p3_x)\n",
    "    p3_x = Conv2D(filters=f3, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(p3_x)\n",
    "    p3_x = ReLU()(p3_x)\n",
    "\n",
    "    if not del_5x5:\n",
    "        p5_x = Conv2D(filters=f5_reduce, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(x)\n",
    "        p5_x = ReLU()(p5_x)\n",
    "        p5_x = Conv2D(filters=f5, kernel_size=(5, 5), strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(p5_x)\n",
    "        p5_x = ReLU()(p5_x)\n",
    "\n",
    "    if add_7x7:\n",
    "        p7_x = Conv2D(filters=f7_reduce, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(x)\n",
    "        p7_x = ReLU()(p7_x)\n",
    "        p7_x = Conv2D(filters=f7, kernel_size=(7, 7), strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(p7_x)\n",
    "        p7_x = ReLU()(p7_x)  \n",
    "\n",
    "    pool_x = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
    "    pool_x = Conv2D(filters=pool_reduce, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(pool_x)\n",
    "    pool_x = ReLU()(pool_x)\n",
    "\n",
    "    if del_5x5 and add_7x7:\n",
    "        x = concatenate(inputs=[p1_x, p3_x, p7_x, pool_x], axis=-1)\n",
    "    elif del_5x5 and not add_7x7:\n",
    "        x = concatenate(inputs=[p1_x, p3_x, pool_x], axis=-1)\n",
    "    elif not del_5x5 and add_7x7:\n",
    "        x = concatenate(inputs=[p1_x, p3_x, p5_x, p7_x, pool_x], axis=-1)\n",
    "    elif not del_5x5 and not add_7x7:\n",
    "        x = concatenate(inputs=[p1_x, p3_x, p5_x, pool_x], axis=-1)\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "# Define the GoogLeNet architecture\n",
    "def create_model(config):\n",
    "\n",
    "    reg_alpha = config['reg_alpha']\n",
    "    del_5x5 = config['del_5x5']\n",
    "    add_7x7 = config['add_7x7']\n",
    "\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same', kernel_regularizer=l2(reg_alpha))(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "            \n",
    "    x = Conv2D(filters=64, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(filters=192, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    x = InceptionBlock(x, 64, 96, 128, 16, 32, 32, reg_alpha, del_5x5, add_7x7)\n",
    "    x = InceptionBlock(x, 128, 128, 192, 32, 96, 64, reg_alpha, del_5x5, add_7x7)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    x = InceptionBlock(x, 192, 96, 208, 16, 48, 64, reg_alpha, del_5x5, add_7x7)\n",
    "\n",
    "    # Auxilary loss-output\n",
    "    x_aux1 = AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='same')(x)\n",
    "    x_aux1 = Conv2D(filters=128, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(x_aux1)\n",
    "    x_aux1 = ReLU()(x_aux1)\n",
    "    x_aux1 = Flatten()(x_aux1)\n",
    "    x_aux1 = Dense(1024, kernel_regularizer=l2(reg_alpha))(x_aux1)\n",
    "    x_aux1 = ReLU()(x_aux1)\n",
    "    x_aux1 = Dropout(0.7)(x_aux1)\n",
    "    x_aux1 = Dense(10)(x_aux1)\n",
    "    x_aux1 = Softmax(name='aux1_out')(x_aux1)\n",
    "\n",
    "    x = InceptionBlock(x, 160, 112, 224, 24, 64, 64, reg_alpha, del_5x5, add_7x7)\n",
    "    x = InceptionBlock(x, 128, 128, 256, 24, 64, 64, reg_alpha, del_5x5, add_7x7)\n",
    "    x = InceptionBlock(x, 112, 144, 288, 32, 64, 64, reg_alpha, del_5x5, add_7x7)\n",
    "\n",
    "    # Auxilary loss-output\n",
    "    x_aux2 = AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='same')(x)\n",
    "    x_aux2 = Conv2D(filters=128, kernel_size=(1, 1), strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(x_aux2)\n",
    "    x_aux2 = ReLU()(x_aux2)\n",
    "    x_aux2 = Flatten()(x_aux2)\n",
    "    x_aux2 = Dense(1024, kernel_regularizer=l2(reg_alpha))(x_aux2)\n",
    "    x_aux2 = ReLU()(x_aux2)\n",
    "    x_aux2 = Dropout(0.7)(x_aux2)\n",
    "    x_aux2 = Dense(10)(x_aux2)\n",
    "    x_aux2 = Softmax(name='aux2_out')(x_aux2)\n",
    "\n",
    "    x = InceptionBlock(x, 256, 160, 320, 32, 128, 128, reg_alpha, del_5x5, add_7x7)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    x = InceptionBlock(x, 256, 160, 320, 32, 128, 128, reg_alpha, del_5x5, add_7x7)\n",
    "    x = InceptionBlock(x, 384, 192, 384, 48, 128, 128, reg_alpha, del_5x5, add_7x7)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(10)(x)\n",
    "    x = Softmax(name='og_out')(x)\n",
    "\n",
    "    outputs = [x_aux1, x_aux2, x]\n",
    "\n",
    "    model = Model(inputs, outputs, name='GoogLeNet')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'val_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'augmentation': {\n",
    "          'values': ['none', 'light', 'heavy']\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({\n",
    "    'earlystopping_patience': {\n",
    "        'value': 10},\n",
    "    'epochs': {\n",
    "        'value': 100},\n",
    "    'learning_rate': {\n",
    "        'value': 0.001},\n",
    "    'batch_size': {\n",
    "        'value': 64},\n",
    "    'kernel_size': {\n",
    "          'value': '5x5'},\n",
    "    'net_filter_size': {\n",
    "          'value': 32},\n",
    "    'net_n': {\n",
    "          'value': 3},\n",
    "    'reg_alpha': {\n",
    "          'value': 0.0001},\n",
    "    'normalization': {\n",
    "          'value': False}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, Lambda, Add, Input, GlobalAveragePooling2D, Flatten, Dense, Softmax\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def ResidualBlock(x, filter_size, is_switch_block, kernel_size, reg_alpha):\n",
    "\n",
    "    # note that if is_switch_block true, it means that output will not be the same as the input\n",
    "    # so while merging the residual connection, we need to adapt to it\n",
    "    # this adaptation could be with a conv layer, or a simple downsampling + padding is enough.\n",
    "\n",
    "    x_skip = x # save original input to the block\n",
    "\n",
    "    if not is_switch_block:\n",
    "        x = Conv2D(filter_size, kernel_size=kernel_size, strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(x)\n",
    "    else:\n",
    "        x = Conv2D(filter_size, kernel_size=kernel_size, strides=(2, 2), padding='same', kernel_regularizer=l2(reg_alpha))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = Conv2D(filter_size, kernel_size=kernel_size, strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if is_switch_block: # takes every second element to half(v) spatial dimension and then adds padding to each side for matching filter (last) dimension\n",
    "        x_skip = Lambda(lambda x: tf.pad(x[:, ::2, ::2, :], tf.constant([[0, 0,], [0, 0], [0, 0], [filter_size//4, filter_size//4]]), mode=\"CONSTANT\"))(x_skip)\n",
    "\n",
    "    x = Add()([x, x_skip])\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def ResidualBlocks(x, filter_size, n, kernel_size, reg_alpha):\n",
    "    for group in range(3): # a stack of 6n layers, 3Ã—3 convolutions, feature maps of sizes {4fs, 2fs, fs}, 2n layers for each size\n",
    "        for block in range(n):\n",
    "            if group > 0 and block == 0: # double filter size\n",
    "                filter_size *= 2\n",
    "                is_switch_block = True\n",
    "            else:\n",
    "                is_switch_block = False\n",
    "                \n",
    "            x = ResidualBlock(x, filter_size, is_switch_block, kernel_size, reg_alpha)\n",
    "\n",
    "    return x\n",
    "\n",
    "# Define the ResNet architecture\n",
    "def create_model(config):\n",
    "\n",
    "    filter_size = config['net_filter_size']\n",
    "    n = config['net_n']\n",
    "    kernel_size = (3, 3) if config['kernel_size'] == '3x3' else (5, 5)\n",
    "\n",
    "    reg_alpha = config['reg_alpha']\n",
    "\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    x = Conv2D(filter_size, kernel_size=kernel_size, strides=(1, 1), padding='same', kernel_regularizer=l2(reg_alpha))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = ResidualBlocks(x, filter_size, n, kernel_size, reg_alpha)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(10)(x)\n",
    "    outputs = Softmax()(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name=f\"ResNet-{n*6+2}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W&B creating sweep (for the grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"CIFAR-10_Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for VGG, others are similar too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train(config = None):\n",
    "    with wandb.init(config=config):\n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "        do_normalization = config['normalization']\n",
    "        do_augmentation = config['augmentation'] != 'none'\n",
    "\n",
    "        x_train_to_use = (x_train.astype('float32') / 255) if do_normalization else x_train\n",
    "        x_test_to_use = (x_test.astype('float32') / 255) if do_normalization else x_test\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = create_model(config[\"kernel_size\"], config[\"dropout\"], config[\"pooling\"], config[\"batchnorm\"], config[\"a_layers\"], config[\"reg_alpha\"])\n",
    "        model.compile(\n",
    "            optimizer = Adam(learning_rate=config[\"learning_rate\"]),\n",
    "            loss = \"categorical_crossentropy\",\n",
    "            metrics = [\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top@3_accuracy')]\n",
    "        )\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                    patience=config[\"earlystopping_patience\"],\n",
    "                                    restore_best_weights=True)\n",
    "\n",
    "        if not do_augmentation:\n",
    "            history = model.fit(x_train_to_use, y_train,\n",
    "                                epochs=config[\"epochs\"],\n",
    "                                batch_size=config[\"batch_size\"],\n",
    "                                validation_split=0.1,\n",
    "                                callbacks=[\n",
    "                                    WandbMetricsLogger(log_freq='epoch'),\n",
    "                                    early_stopping\n",
    "                                ], verbose=1\n",
    "                                )\n",
    "        else:\n",
    "            if config['augmentation'] == 'light':\n",
    "                datagen = ImageDataGenerator(\n",
    "                    rotation_range=20,\n",
    "                    horizontal_flip=True,\n",
    "                    width_shift_range=0.1,\n",
    "                    height_shift_range=0.1,\n",
    "                    fill_mode='nearest'\n",
    "                )\n",
    "            else:\n",
    "                datagen = ImageDataGenerator(\n",
    "                    rotation_range=40,\n",
    "                    horizontal_flip=True,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    shear_range=0.1,\n",
    "                    zoom_range=0.1,\n",
    "                    fill_mode='nearest'\n",
    "                )\n",
    "\n",
    "            x_tr, x_vl, y_tr, y_vl = train_test_split(x_train_to_use, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "            train_datagen = datagen.flow(x_tr, y_tr, batch_size=config[\"batch_size\"])\n",
    "            history = model.fit(train_datagen,\n",
    "                                epochs=config[\"epochs\"],\n",
    "                                batch_size=config[\"batch_size\"],\n",
    "                                validation_data=(x_vl, y_vl),\n",
    "                                callbacks=[\n",
    "                                    WandbMetricsLogger(log_freq='epoch'),\n",
    "                                    early_stopping\n",
    "                                ], verbose=1\n",
    "                                )\n",
    "            \n",
    "        \n",
    "        test_stats = model.evaluate(x_test_to_use, y_test)\n",
    "        wandb.log({\"test_loss\": test_stats[0]})\n",
    "        wandb.log({\"test_acc\": test_stats[1]})\n",
    "\n",
    "        val_loss_history = history.history['val_loss']\n",
    "        val_acc_history = history.history['val_accuracy']\n",
    "\n",
    "        best_epoch_num = -1 if (len(val_loss_history) == 100 or len(val_loss_history) <= 10) else (len(val_loss_history) - 11)\n",
    "\n",
    "        wandb.log({\"best_val_loss\": val_loss_history[best_epoch_num]})\n",
    "        wandb.log({\"best_val_acc\": val_acc_history[best_epoch_num]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
